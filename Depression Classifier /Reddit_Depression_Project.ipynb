{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jFvbbC6VtZm"
      },
      "source": [
        "# Reddit Depression Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoBxKQ_OVl-j",
        "outputId": "c32cbead-2222-4919-baa9-9fcbb07f07dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: happiestfuntokenizing in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install happiestfuntokenizing\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selectiotn import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
        "from collections import Countero\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# FILEPATH = 'drive/MyDrive/data/'\n",
        "FILEPATH = 'drive/MyDrive/Brown Stuff/Computational Linguistics/Assignments/Final Project/student.pkl'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMOTL7mV9T9"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "outputs": [],
      "source": [
        "def load(depression_subreddits, lda):\n",
        "  \"\"\"\n",
        "    Function that loads the dataset with all depression related posts and the control posts. It also\n",
        "    tokenizes and does stop word removal (but only for lda).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    depression_subreddits : list[str]\n",
        "        the list of all depression related subreddits\n",
        "    lda : bool\n",
        "        a boolean to determine if the dataset is needed for lda (ensures tokenizing and stop word removal only done for lda)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    a dataframe with all post's (including the control and all symptoms) text (under post) and subreddit (under symptom)\n",
        "  \"\"\"\n",
        "\n",
        "  CACHE_PATH = f\"{FILEPATH}_{'lda' if lda else 'roBERTa'}_processed.pickle\"\n",
        "\n",
        "  #if the cache_path exists that means open the contents of the pickle file\n",
        "  if os.path.exists(CACHE_PATH):\n",
        "      datasets = pickle.load(open(CACHE_PATH, \"rb\"))\n",
        "      return datasets\n",
        "  #else dump the posts into a pickle file\n",
        "  elif lda:\n",
        "  #generate the original dataset and tokenize\n",
        "      datasets = dataset_generation(depression_subreddits)\n",
        "\n",
        "      #only preprocesses for lda\n",
        "      if lda:\n",
        "        tokenized_posts_dict = {\n",
        "          symptom: [tokenize(post) for post in posts]\n",
        "          for symptom, posts in datasets.items()\n",
        "        }\n",
        "\n",
        "        #generate stop_words\n",
        "        stop_words_data = stop_words(tokenized_posts_dict)\n",
        "        stop_words_data = stop_words_data.get(\"Control\")\n",
        "\n",
        "        stop_words_list = []\n",
        "        for posts in stop_words_data:\n",
        "          stop_words_list.append(posts[0])\n",
        "\n",
        "        #remove stop words from symptom\n",
        "        for symptoms, posts in tokenized_posts_dict.items():\n",
        "          for index, post in enumerate(posts):\n",
        "            stop_word_post = []\n",
        "            for word in post:\n",
        "              if word not in stop_words_list:\n",
        "                stop_word_post.append(word)\n",
        "              else:\n",
        "                stop_word_post.append(\"<OOV>\")\n",
        "            tokenized_posts_dict[symptoms][index] = stop_word_post\n",
        "\n",
        "      #converts symptom dictionary into a dataframe\n",
        "      symptom_data = []\n",
        "      for symptom, posts in tokenized_posts_dict.items():\n",
        "          symptom_data.extend([{\n",
        "              \"symptom\": symptom,\n",
        "              \"post\": post\n",
        "          } for post in posts])\n",
        "\n",
        "      symptom_df = pd.DataFrame(symptom_data)\n",
        "\n",
        "\n",
        "      # Save the combined data into a pickle file\n",
        "      pickle.dump(symptom_df, open(CACHE_PATH, \"wb\"))\n",
        "      return symptom_df\n",
        "\n",
        "  else:\n",
        "    # for non-lda (roBERTa)\n",
        "    datasets = dataset_generation(depression_subreddits)\n",
        "\n",
        "    #skip preprocess and load straight into dataframe\n",
        "    symptom_data = []\n",
        "\n",
        "    for symptom, posts in datasets.items():\n",
        "        symptom_data.extend([{\n",
        "            \"symptom\": symptom,\n",
        "            \"post\": post\n",
        "        } for post in posts])\n",
        "\n",
        "    symptom_df = pd.DataFrame(symptom_data)\n",
        "\n",
        "\n",
        "    # Save the combined data into a pickle file\n",
        "    pickle.dump(symptom_df, open(CACHE_PATH, \"wb\"))\n",
        "\n",
        "    return symptom_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wpw9kJiras4B"
      },
      "outputs": [],
      "source": [
        "def dataset_generation(depression_subreddits):\n",
        "  \"\"\"\n",
        "    Function that generates the datasets, finding all posts that correlate to a depression related subreddit or meet control requirements\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    depression_subreddits : list[str]\n",
        "        the list of all depression related subreddits\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    returns a dictionary with the key being the specific depression related subreddit or 'Control' and the value being a list of the actual text of every\n",
        "    post related to that subreddit/'Control'\n",
        "  \"\"\"\n",
        "\n",
        "  #open initial file and pull information related to each category\n",
        "  with open(FILEPATH, 'rb') as f:\n",
        "    df = pd.read_pickle(f)\n",
        "    reddit_posts = df['text']\n",
        "    reddit_authors = df['author']\n",
        "    subreddit_cats = df['subreddit']\n",
        "    post_utc_times = df['created_utc']\n",
        "    post_dates = df['date']\n",
        "\n",
        "  control_dataset = []\n",
        "  symptom_dataset_dict = {}\n",
        "\n",
        "  #input author's name, returns UTC of oldest post\n",
        "  author_oldest_post_dict = {}\n",
        "\n",
        "  #input author's name, get UTC of every post\n",
        "  author_to_post_UTC_dict = {}\n",
        "\n",
        "  #creating empty lists for each depression category\n",
        "  for symptoms in depression_subreddits:\n",
        "    symptom_dataset_dict[symptoms] = []\n",
        "  symptom_dataset_dict[\"Control\"] = []\n",
        "\n",
        "  #appends to symptom dataset\n",
        "  #for every post\n",
        "  for index in range(len(reddit_posts)):\n",
        "    #checks if the specific post has a category and if the category is a depression subreddit\n",
        "    if index in subreddit_cats.index:\n",
        "      if subreddit_cats[index] in depression_subreddits:\n",
        "        #if the author is already a key in the dictionary, just append the utc for all the posts, else also add the key\n",
        "        if reddit_authors[index] in author_to_post_UTC_dict:\n",
        "          author_to_post_UTC_dict[reddit_authors[index]].append(post_utc_times[index])\n",
        "        else:\n",
        "          author_to_post_UTC_dict[reddit_authors[index]] = [post_utc_times[index]]\n",
        "\n",
        "        #add to symptom subreddit\n",
        "        current_symptom = subreddit_cats[index]\n",
        "        symptom_dataset_dict[current_symptom].append(reddit_posts[index])\n",
        "\n",
        "  i = 0\n",
        "  #appends to control dataset\n",
        "  for index in range(len(reddit_posts)):\n",
        "    #verify that the specific category exists and is not a depression topic\n",
        "    if index in subreddit_cats.index:\n",
        "      if subreddit_cats[index] not in depression_subreddits:\n",
        "        i+=1\n",
        "        # verify at least six months have passed between control and depression post\n",
        "        if six_months_passed(post_utc_times[index], reddit_authors[index], author_oldest_post_dict, author_to_post_UTC_dict):\n",
        "          #append post to control dataset\n",
        "          symptom_dataset_dict[\"Control\"].append(reddit_posts[index])\n",
        "          # control_dataset.append(reddit_posts[index])\n",
        "\n",
        "\n",
        "  # return (control_dataset, symptom_dataset_dict)\n",
        "  return symptom_dataset_dict\n",
        "\n",
        "def six_months_passed(utc: int, author_name, author_oldest_post_dict, author_to_post_UTC_dict):\n",
        "  \"\"\"\n",
        "    this helper method for control dataset generation returns a boolean depending on whether the control post is 180 days\n",
        "    older than the mental health one\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    utc : int\n",
        "      the utc of the control post candidate\n",
        "\n",
        "    author_name : str\n",
        "      the name of the author\n",
        "\n",
        "    author_oldest_post_dict : dict\n",
        "      key is the author's name, value is the utc of the oldest post\n",
        "\n",
        "    author_to_post_UTC_dict : dict\n",
        "      key is the author's name, value is the utc of every post\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    returns a boolean telling you whether the control post is 180 days older than the mental health one\n",
        "  \"\"\"\n",
        "  #use a dictionary to find the author's oldest post UTC\n",
        "  oldest_author_post = author_oldest_post_dict.get(author_name)\n",
        "\n",
        "  #will only update dictionary to find an author's new post if its a new author\n",
        "  if not oldest_author_post:\n",
        "    oldest_author_post = find_oldest_depression_post(author_name, author_to_post_UTC_dict)\n",
        "    author_oldest_post_dict[author_name] = oldest_author_post\n",
        "\n",
        "  #tells you if 180 days have passed between the mental health and non-mental health post\n",
        "  if oldest_author_post != None:\n",
        "    if oldest_author_post:\n",
        "      if oldest_author_post - utc >= 15552000:\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "#this method finds the oldest depression post for a specific author\n",
        "def find_oldest_depression_post(name, author_to_post_UTC_dict):\n",
        "  \"\"\"\n",
        "    helper method for six_months_passed() method that finds the oldest depression post UTC\n",
        "\n",
        "  \"\"\"\n",
        "  #gets all the UTC times of every post a specific author made\n",
        "  UTC_times = author_to_post_UTC_dict.get(name)\n",
        "\n",
        "  #sorts every post to find the lowest UTC (oldest depression post) and returns it (or None if an author has 0 depression posts)\n",
        "  if UTC_times:\n",
        "    UTC_times.sort()\n",
        "    return UTC_times[0]\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ohOK3wCdWpnA"
      },
      "outputs": [],
      "source": [
        "# List of depression subreddits\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MWGVUju_WxuP"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def tokenize(unprocessed_posts):\n",
        "  \"\"\"\n",
        "    input the posts and tokenizes the post for lda dataset\n",
        "\n",
        "  \"\"\"\n",
        "  # tokenizes posts\n",
        "  tokenizer = Tokenizer()\n",
        "  tokens = tokenizer.tokenize(unprocessed_posts)\n",
        "\n",
        "  # list of all unqiue punctuation tokens\n",
        "  punctuation = set(string.punctuation)\n",
        "\n",
        "  # Filter out the punctuation tokens\n",
        "  filtered_tokens = [token for token in tokens if token not in punctuation]\n",
        "  return filtered_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q3j9z7UuW3eG"
      },
      "outputs": [],
      "source": [
        "def stop_words(datasets):\n",
        "  \"\"\"\n",
        "    finds the 100 most common stop words for each symptom and control (technically only control is neccessary)\n",
        "    and puts them into a dict\n",
        "\n",
        "  \"\"\"\n",
        "  # get 100 most common words in control\n",
        "  most_common_word_dict = {}\n",
        "\n",
        "  # for key, values in symptoms_dataset_dict.items():\n",
        "  for key, values in datasets.items():\n",
        "    # Iterate through the dictionary and update the Counter with words from each list\n",
        "    word_counter = Counter()\n",
        "    for words in values:\n",
        "      word_counter.update(words)\n",
        "\n",
        "    # Find the 100 most common words across all keys\n",
        "    most_common_words = word_counter.most_common(100)\n",
        "\n",
        "    most_common_word_dict[key] = most_common_words\n",
        "\n",
        "  return most_common_word_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# key to map 40+ subreddits to 13 symptoms\n",
        "symptom_mapping = {\n",
        "    'Control': ['Control'],\n",
        "    'Anger': ['Anger'],\n",
        "    'Anhedonia': ['Anhedonia', 'DeadBedrooms'],\n",
        "    'Anxiety': ['Anxiety', 'AnxietyDepression', 'HealthAnxiety', 'PanicAttack'],\n",
        "    'Concentration deficit': ['DecisionMaking', 'shouldi'],\n",
        "    'Disordered eating': ['bingeeating', 'BingeEatingDisorder', 'EatingDisorders', 'eating_disorders', 'EDAnonymous'],\n",
        "    'Fatigue': ['chronicfatigue', 'Fatigue'],\n",
        "    'Loneliness': ['ForeverAlone', 'lonely'],\n",
        "    'Sad mood': ['cry', 'grief', 'sad', 'Sadness'],\n",
        "    'Self-loathing': ['AvPD', 'SelfHate', 'selfhelp', 'socialanxiety', 'whatsbotheringyou'],\n",
        "    'Sleep problem': ['insomnia', 'sleep'],\n",
        "    'Somatic complaint': ['cfs', 'ChronicPain', 'Constipation', 'EssentialTremor', 'headaches', 'ibs', 'tinnitus'],\n",
        "    'Suicidal thought and attempts': ['AdultSelfHarm', 'selfharm', 'SuicideWatch'],\n",
        "    'Worthlessness': ['Guilt', 'Pessimism', 'selfhelp', 'whatsbotheringyou']\n",
        "}\n",
        "\n",
        "# actually maps subreddits to symptoms\n",
        "def map_subreddit_to_symptom(subreddit, symptom_mapping):\n",
        "    # if a subreddit maps to the symptom, return the symptom\n",
        "    for symptom, subreddits in symptom_mapping.items():\n",
        "        if subreddit in subreddits:\n",
        "            return symptom\n",
        "    # Return 'Other' if subreddit doesn't correlate to symptom\n",
        "    return 'Other'"
      ],
      "metadata": {
        "id": "KWeMltThgD62"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0-97hsVXNkF"
      },
      "source": [
        "## RoBERTa Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the roberta dataset (no tokenizing or stop word removal)\n",
        "roberta_dataset = load(depression_subreddits, False)"
      ],
      "metadata": {
        "id": "wmB2CSjdF_EW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(input_text):\n",
        "    \"\"\"\n",
        "    helper method for run_roberta() that allows you to input your dataset and it gives the averaged word embeddings for all posts\n",
        "    in control and the symptom datasets\n",
        "\n",
        "    \"\"\"\n",
        "    # intialize tokenizer and model\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
        "    model = RobertaModel.from_pretrained('distilroberta-base')\n",
        "\n",
        "    # enables model to use GPU and prevents model from crashing\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # average embeddings for each posts (list)\n",
        "    avg_embeddings = []\n",
        "\n",
        "    # for every post\n",
        "    for post in input_text['post'].tolist():\n",
        "\n",
        "      # tokenize the post then send to GPU\n",
        "      encoded_posts = tokenizer(post, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "      encoded_posts.to(device)\n",
        "\n",
        "      # input tokenized words into model\n",
        "      with torch.no_grad():\n",
        "        outputs = model(**encoded_posts)\n",
        "\n",
        "      # extract word embeddings from the last hidden state and average the word embeddings to get an average post embedding\n",
        "      last_hidden_state = outputs.last_hidden_state\n",
        "      avg_embedding = last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "      avg_embeddings.append(avg_embedding)\n",
        "\n",
        "    # cast this list into a numpy array and return\n",
        "    numpy_embeddings = np.array(avg_embeddings)\n",
        "    return numpy_embeddings"
      ],
      "metadata": {
        "id": "nezQZmSZhfOo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "blx1SWVMXYDp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "def run_roberta():\n",
        "  \"\"\"\n",
        "    runs roberta, getting the embeddings from the helper method above, and putting the embeddings in X and actual symptoms in y and\n",
        "    running with main to output accuracies by symptom\n",
        "\n",
        "  \"\"\"\n",
        "  # prevents the need to get embeddings each time with a pickle file\n",
        "  CACHE_PATH = f\"{FILEPATH}_embeddings.pickle\"\n",
        "\n",
        "  if os.path.exists(CACHE_PATH):\n",
        "    avg_embeddings = pickle.load(open(CACHE_PATH, \"rb\"))\n",
        "  else:\n",
        "    embeddings = get_embeddings(roberta_dataset)\n",
        "    pickle.dump(embeddings, open(CACHE_PATH, \"wb\"))\n",
        "    avg_embeddings = embeddings\n",
        "\n",
        "  # maps posts by subreddit to the correct symptom (ignore the 'symptom' designation that's acutaally the subreddit col)\n",
        "  roberta_dataset['grouped_symptom'] = roberta_dataset['symptom'].apply(lambda x: map_subreddit_to_symptom(x, symptom_mapping))\n",
        "\n",
        "  # get the list of all symptoms\n",
        "  symptom_list = roberta_dataset['grouped_symptom'].tolist()\n",
        "\n",
        "  # Iterate through the symptom list and get a dictionary with the keys being each specific symptom + control and the values returned being\n",
        "  # the index of each post corresponding to the symptom/control\n",
        "  symptom_groups = {}\n",
        "\n",
        "  for index, post_symptom in enumerate(symptom_list):\n",
        "      symptom_groups.setdefault(post_symptom, []).append(index)\n",
        "\n",
        "  print(\"roBERTa Outputs (accuracy is in decimal form): \")\n",
        "  desired_symptoms = ['Worthlessness', 'Anger', 'Anhedonia', 'Anxiety', 'Disordered eating', 'Loneliness', 'Sad mood', 'Self-loathing', 'Sleep problem', 'Somatic complaint']\n",
        "\n",
        "  # for each symptom you want\n",
        "  for symptom in desired_symptoms:\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # combine the specific symptom and control posts\n",
        "    combined_list = symptom_groups.get(symptom) + symptom_groups.get(\"Control\")\n",
        "\n",
        "    # for each post, add the speicific symptom as y and the word embedding as X\n",
        "    for index in combined_list:\n",
        "      y.append(symptom_list[index])\n",
        "      X.append(avg_embeddings[index])\n",
        "\n",
        "    # convert y into a binary with 0 meaning Control and 1 meaning the specific symptom\n",
        "    new_y = []\n",
        "    for y_symptom in y:\n",
        "      if y_symptom == 'Control':\n",
        "        new_y.append(0)\n",
        "      else:\n",
        "        new_y.append(1)\n",
        "\n",
        "    # case X and y into numpy arrays\n",
        "    X = np.array(X)\n",
        "    X = X.squeeze(axis=1)\n",
        "    y = np.array(new_y)\n",
        "\n",
        "    # run main to evaluate and print each symptom\n",
        "    accuracy_by_symptom = main(X, y)\n",
        "\n",
        "    print(symptom + \": \" + str(accuracy_by_symptom))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4I37U1SXAEZ"
      },
      "source": [
        "## Reddit Topics with LDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xf3surfWXH-q"
      },
      "outputs": [],
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "\n",
        "def run_lda_on_all(datasets):\n",
        "    \"\"\"\n",
        "    simply just runs lda on all posts in the dataset, returning the model, dictionary (idx2word), and corpus (combo of topics per doc)\n",
        "\n",
        "    \"\"\"\n",
        "    # make a list of all posts and check they aren't blank\n",
        "    all_posts = datasets['post'].tolist()\n",
        "\n",
        "    valid_posts = []\n",
        "    for post in all_posts:\n",
        "      if post and len(post) > 0:\n",
        "        valid_posts.append(post)\n",
        "\n",
        "    # Create idx2word for all posts\n",
        "    idx2word = Dictionary(valid_posts)\n",
        "\n",
        "    # Create corpus (bag of words) for each post\n",
        "    corpus = []\n",
        "    for post in valid_posts:\n",
        "      corpus.append(idx2word.doc2bow(post))\n",
        "\n",
        "    # run lda\n",
        "    lda_model = LdaMulticore(corpus=corpus, id2word=idx2word, num_topics=200, passes=2)\n",
        "\n",
        "    # Return the LDA model, dictionary, and corpus\n",
        "    lda_outputs = {'model': lda_model,'dictionary': idx2word,'corpus': corpus}\n",
        "\n",
        "    return lda_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iIltaNYuwDns"
      },
      "outputs": [],
      "source": [
        "def run_lda():\n",
        "  \"\"\"\n",
        "    runs the actual lda, getting the lda outputs from the helper method above, and converting it into topic distributions you put into X and symptoms\n",
        "    you put into y for the main function, then returning the accuracy of each symptom\n",
        "\n",
        "  \"\"\"\n",
        "  # load the lda dataset (including tokenizing and stop word removal)\n",
        "  lda_datasets = load(depression_subreddits, True)\n",
        "\n",
        "  # assigns the subreddits to the symptoms (ignore the 'symptom' designation that's acutaally the subreddit col)\n",
        "  lda_datasets['grouped_symptom'] = lda_datasets['symptom'].apply(lambda x: map_subreddit_to_symptom(x, symptom_mapping))\n",
        "\n",
        "  #prevents LDA from having to run every time with a pickle file\n",
        "  CACHE_PATH = f\"{FILEPATH}_lda_results.pickle\"\n",
        "\n",
        "  if os.path.exists(CACHE_PATH):\n",
        "    lda_results = pickle.load(open(CACHE_PATH, \"rb\"))\n",
        "  #else dump the posts into a pickle file\n",
        "  else:\n",
        "    lda_results = run_lda_on_all(lda_datasets)\n",
        "    pickle.dump(lda_results, open(CACHE_PATH, \"wb\"))\n",
        "    lda_results = lda_results\n",
        "\n",
        "  print(\"LDA Outputs: \")\n",
        "  desired_symptoms = ['Worthlessness', 'Anger', 'Anhedonia', 'Anxiety', 'Disordered eating', 'Loneliness', 'Sad mood', 'Self-loathing', 'Sleep problem', 'Somatic complaint']\n",
        "  #shuffle dataset to improve model accuracy\n",
        "  lda_datasets = lda_datasets.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "  # run lda for each symptom\n",
        "  for symptom in desired_symptoms:\n",
        "\n",
        "    #filter datasets just for specific symptom and control\n",
        "    control_data = lda_datasets[lda_datasets['grouped_symptom'] == 'Control']\n",
        "    symptom_data = lda_datasets[lda_datasets['grouped_symptom'] == symptom]\n",
        "\n",
        "    # Combine the control and specific symptom data into one dataset and shuffle\n",
        "    lda_datasets_filtered = pd.concat([symptom_data, control_data])\n",
        "    lda_datasets_filtered = lda_datasets_filtered.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # creates a new corpus only with posts from the filtered dataset (the specific symptom + control)\n",
        "    filtered_corpus = []\n",
        "    all_filtered_posts = lda_datasets_filtered['post'].tolist()\n",
        "\n",
        "    for post in all_filtered_posts:\n",
        "        filtered_corpus.append(lda_results.get(\"dictionary\").doc2bow(post))\n",
        "\n",
        "\n",
        "    # get topic distributions and convert it into a numpy array which is a matrix of topic vectors\n",
        "    topic_distributions = [lda_results.get(\"model\").get_document_topics(doc) for doc in filtered_corpus]\n",
        "    X = convert_to_topic_matrix(topic_distributions, num_topics=200)\n",
        "\n",
        "    # prepare answer key to evaluate model (0 is \"Control\" and 1 is specific symptoms)\n",
        "    y = lda_datasets_filtered['grouped_symptom'].apply(lambda x: 0 if x == 'Control' else 1).values\n",
        "\n",
        "    # run main to get specific symptoms and print\n",
        "    accuracy_by_symptom = main(X, y)\n",
        "    print(symptom + \": \" + str(accuracy_by_symptom))\n",
        "\n",
        "def convert_to_topic_matrix(topic_distributions, num_topics=200):\n",
        "\n",
        "    \"\"\"\n",
        "    For each post in the filtered corpus, create a 200 length zero vector and assign topic probabilities to each topic, return a numpy\n",
        "    array with the topic vector for each doc\n",
        "    \"\"\"\n",
        "\n",
        "    padded_topic_matrix = []\n",
        "\n",
        "    # for each post, create an all 0 vector of length 200 topics (total number of topics)\n",
        "    for post in topic_distributions:\n",
        "        all_zero_vector = [0] * num_topics\n",
        "        # fill in the topic probabilities that appear in the doc\n",
        "        for topic, topic_prob in post:\n",
        "            all_zero_vector[topic] = topic_prob\n",
        "        padded_topic_matrix.append(all_zero_vector)\n",
        "    #return a numpy array that has the probability of each topic for each post\n",
        "    return np.array(padded_topic_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWxuF2jXtwi"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "koTBPhcDXujb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493b2b59-8f2c-447b-cdeb-0e68c84954b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roBERTa Outputs (accuracy is in decimal form): \n",
            "Worthlessness: 0.8272385771235817\n",
            "Anger: 0.9331138258810611\n",
            "Anhedonia: 0.9055510874886356\n",
            "Anxiety: 0.9151186024247584\n",
            "Disordered eating: 0.9263507513419804\n",
            "Loneliness: 0.8697202356709411\n",
            "Sad mood: 0.868400837988827\n",
            "Self-loathing: 0.8785214922278957\n",
            "Sleep problem: 0.936635594804224\n",
            "Somatic complaint: 0.8784724103899423\n",
            "\n",
            "\n",
            "LDA Outputs: \n",
            "Worthlessness: 0.6605677023398542\n",
            "Anger: 0.8195432177128358\n",
            "Anhedonia: 0.8490962280040778\n",
            "Anxiety: 0.8355403933539782\n",
            "Disordered eating: 0.8708661513290473\n",
            "Loneliness: 0.7637219222764429\n",
            "Sad mood: 0.7641562535179556\n",
            "Self-loathing: 0.7716024286073371\n",
            "Sleep problem: 0.9004115293058883\n",
            "Somatic complaint: 0.7764582002436976\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import warnings\n",
        "\n",
        "\n",
        "def main(X, y):\n",
        "    \"\"\"\n",
        "    Runs 5-fold cross-validation with Random Forest to evaluate your LDA performance.\n",
        "    \"\"\"\n",
        "    rf_classifier = RandomForestClassifier()\n",
        "    cv = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "    # stores the models outputs as well as the answer key\n",
        "    model_predictions = []\n",
        "    actual_results = []\n",
        "\n",
        "    # Run cross-validation\n",
        "    for training, testing in cv.split(X):\n",
        "\n",
        "        # Split X and y into train and test sets\n",
        "        X_train = X[training]\n",
        "        X_test = X[testing]\n",
        "        y_train = y[training]\n",
        "        y_test = y[testing]\n",
        "\n",
        "        # Train classifier and get data\n",
        "        rf_classifier.fit(X_train, y_train)\n",
        "        predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "        # Store the predictions and the answer key\n",
        "        model_predictions.extend(predictions)\n",
        "        actual_results.extend(y_test)\n",
        "\n",
        "    # evaluate accuracy by comparing the predictions to the actual result\n",
        "    accuracy_by_symptom = roc_auc_score(model_predictions, actual_results)\n",
        "\n",
        "    # return actual accuracy\n",
        "    return accuracy_by_symptom\n",
        "\n",
        "#print the actual outputs by symptom for lda and roberta\n",
        "run_roberta()\n",
        "print(\"\\n\")\n",
        "run_lda()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}